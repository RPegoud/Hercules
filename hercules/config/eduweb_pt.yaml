experiment:
  name: "eduweb_pt"
  seed: 0
  epochs: 1
  eduweb_batch_size: 8
  batch_size: 4 # used for Babilong
  learning_rate: 4e-4
  weight_decay: 0.1 

  num_train_samples: 1e4 # number of eduweb samples used for training
  num_test_it: 200

  train_task_name: "2k" # the maximal sequence length of the Babilong samples
  test_task_name: "2k"
  train_splits: ~ # Babilong splits used in training
  test_splits: "all" # Babilong splits used in testing
  eval_with_generate: true # whether to evaluate the model performance with `.generate()`
  max_gen_tokens: 50 # maximum number of tokens generated for evaluation

  save_model: false
  log_experiment: false

neural_memory:
  mlp_depth: 5
  mlp_expansion_factor: 6
  max_adaptive_lr: 1e-2
  n_chunks: 512

memory_llama:
  llama_hf_path: "meta-llama/Llama-3.2-1B"
  memory_layer_id: 0 # the index of the layer to augment with a memory module
  mode: "embedding"
  use_lora: true

lora:
  lora_rank: 16        # The rank of the LoRA matrices.
  lora_alpha: 32       # The scaling factor for the LoRA weights.
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"  
    - "down_proj"  
    - "up_proj" 