experiment:
  name: "eduweb_pt"
  seed: 0
  epochs: 1
  eduweb_train_batch_size: 4
  eduweb_val_batch_size: 4
  babilong_test_batch_size: 1

  gradient_accumulation_steps: 4
  learning_rate: 1e-3
  weight_decay: 0.1
  scheduler: "constant" # ["constant", "cosine"]

  num_train_samples: 5e4 # number of eduweb samples used for training
  num_val_samples: 100 # number of samples used to compute the average validation loss
  num_eval: 50 # number of evaluations during training
  num_test_samples: 100
  max_train_seq_len: 2048 # maximum sequence length of training samples

  train_task_name: "2k" # the maximal sequence length of the Babilong samples
  test_task_name: "2k"
  train_splits: ~ # Babilong splits used in training
  test_splits: "all" # Babilong splits used in testing
  eval_with_generate: true # whether to evaluate the model performance with `.generate()`
  max_gen_tokens: 25 # maximum number of tokens generated for evaluation

  save_every: 100 # frequency of checkpoints
  save_recent_n: 3 # the number of recent checkpoints to save
  log_experiment: false # whether to log the experiment with wandb
  resume_from_checkpoint: true # whether to load an existing model checkpoint
  checkpoint_name: "checkpoints/memory_llama/09-13_08-26/best_loss"
  resume_from_step: ~ # step to restart training from

  run_name: "TEST:Llama1B|ALMA_no_LoRA_2000|MAC_seq|m14_15|constant_lr|t5e4"

neural_memory:
  mlp_depth: 3
  mlp_expansion_factor: 2
  max_adaptive_lr: 1e-2 # maximum adaptive learning rate

memory_llama:
  memory_arch: "context" # the titans architecture to inject, one of ["layer", "context"]
  llama_hf_path: "meta-llama/Llama-3.2-1B"
  memory_layer_id: [14, 15] # the index of the layer to augment with a memory module
  use_lora: false
  track_memory_statistics: true
  
lora:
  lora_rank: 16        # The rank of the LoRA matrices.
  lora_alpha: 32       # The scaling factor for the LoRA weights.
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "down_proj"
    - "up_proj"