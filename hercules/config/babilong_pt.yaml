experiment:
  name: "babilong_pt"
  seed: 0

  epochs: 1
  batch_size: 8
  learning_rate: 4e-4
  weight_decay: 0.1 
  
  train_task_name: "2k" # the maximal sequence length of the Babilong samples
  test_task_name: "2k"
  train_splits: ["qa1", "qa5", "qa7", "qa8"] # splits used in training
  test_splits: "all" # "remaining": all non-train splits, "all": all splits, "qa1": single split, ["qa1", "qa2"]: multiple splits

  use_global_split: true # if true, overrides the train and test splits, aggregating all the splits and using `global_split_test_size` for testing
  global_split_test_size: 0.3

  save_model: false
  save_model_name: "global_split"
  log_experiment: false

neural_memory:
  meta_memory_dim: 256
  max_adaptive_lr: 1e-2
  num_attention_heads: 8
  attention_window_size: 32
  conv_kernel_size: 256
  n_chunks: 128

memory_llama:
  llama_hf_path: "meta-llama/Llama-3.2-1B"
  freeze_llama_layers: true
  memory_layer_id: -2 # the index of layer to augment with a memory module

