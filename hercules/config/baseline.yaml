experiment:
  name: "eduweb_pt"
  seed: 0
  epochs: 1
  eduweb_batch_size: 8
  batch_size: 8 # used for Babilong

  num_train_samples: 1e5 # number of eduweb samples used for training
  max_seq_len: 2048

  train_task_name: "2k" # the maximal sequence length of the Babilong samples
  test_task_name: "2k"
  train_splits: ~ # Babilong splits used in training
  test_splits: "all" # Babilong splits used in testing
  eval_with_generate: true # whether to evaluate the model performance with `.generate()`
  max_gen_tokens: 20 # maximum number of tokens generated for evaluation

  save_model: false
  log_experiment: false

neural_memory:
  meta_memory_dim: 256
  max_adaptive_lr: 1e-2
  num_attention_heads: 8
  attention_window_size: 32
  conv_kernel_size: 256
  n_chunks: 128

memory_llama:
  llama_hf_path: "meta-llama/Llama-3.2-1B"
  freeze_llama_layers: true
  memory_layer_id: -2 # the index of layer to augment with a memory module

