seed: 0
epochs: 1
batch_size: 8
learning_rate: 4e-4
weight_decay: 0.1 
train_task_name: "2k" # the maximal sequence length of the Babilong samples
test_task_name: "2k"
train_splits: ["qa1", "qa5", "qa7", "qa8"] # splits used in training
test_splits: "all" # "remaining": all non-train splits, "all": all splits, "qa1": single split, ["qa1", "qa2"]: multiple splits

use_global_split: true # if true, overrides the train and test splits, aggregating all the splits and using `global_split_test_size` for testing
global_split_test_size: 0.3

save_model: false
save_model_name: "global_split"
log_experiment: false
