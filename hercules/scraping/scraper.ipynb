{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c78f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from colorama import Fore, Style\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba36a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "EXPECTED_MONSTER_FIELDS = [\n",
    "    \"symbol\",\n",
    "    \"Difficulty\",\n",
    "    \"Attacks\",\n",
    "    \"Baselevel\",\n",
    "    \"Baseexperience\",\n",
    "    \"Speed\",\n",
    "    \"BaseAC\",\n",
    "    \"Base MR\",\n",
    "    \"Alignment\",\n",
    "    \"Frequency(bynormal means)\",\n",
    "    \"Genocidable\",\n",
    "    \"Weight\",\n",
    "    \"Nutritional value\",\n",
    "    \"Size\",\n",
    "    \"Resistances\",\n",
    "    \"Resistancesconveyed\",\n",
    "    \"facts\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetHackScraper:\n",
    "    def __init__(self, is_local: bool):\n",
    "        self.root_url = (\n",
    "            \"http://localhost:8080\" if is_local else \"https://nethackwiki.com\"\n",
    "        )\n",
    "        self.base_index_url = (\n",
    "            \"http://localhost:8080/index.php\" if is_local else \"https://nethackwiki.com/wiki\"\n",
    "        )\n",
    "        self.monsters_url = f\"{self.base_index_url}/Monster\"\n",
    "        self.items_url = f\"{self.base_index_url}/Item\"\n",
    "\n",
    "        self.headers = {} if is_local else HEADERS\n",
    "\n",
    "        self._initialise_requests()\n",
    "\n",
    "    def _initialise_requests(self) -> None:\n",
    "        monsters_response = requests.get(self.monsters_url, headers=self.headers)\n",
    "        self.monsters_url_soup = BeautifulSoup(monsters_response.content, \"html.parser\")\n",
    "\n",
    "        items_response = requests.get(self.items_url, headers=self.headers)\n",
    "        self.items_url_soup = BeautifulSoup(items_response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "    # --- Monsters ---\n",
    "    def _get_monsters_url_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"Collects a dictionary of monster urls.\"\"\"\n",
    "        monsters_urls = {}\n",
    "\n",
    "        top_level_lis = self.monsters_url_soup.select(\"ul > li\")\n",
    "        for li in tqdm(top_level_lis):\n",
    "            nested_ul = li.find(\"ul\")\n",
    "            if not nested_ul:\n",
    "                continue  # skip items without a nested list\n",
    "\n",
    "            for sub_li in nested_ul.find_all(\"li\", recursive=False):\n",
    "                # get the second <a> tag (the first is usually the image link)\n",
    "                links = sub_li.find_all(\"a\")\n",
    "                if len(links) >= 2:\n",
    "                    monster_tag = links[1]\n",
    "                    name = monster_tag.get_text(strip=True)\n",
    "                    monster_url_relative = monster_tag.get(\"href\").split(\"wiki\")[-1]\n",
    "                    url = self.root_url + monster_url_relative\n",
    "                    monsters_urls[name] = url\n",
    "\n",
    "        return monsters_urls\n",
    "\n",
    "    def parse_monster_info(self, monster_url:str) -> Dict[str, str | List[str]]:\n",
    "        \"\"\"Collects a dictionary of monster properties (see `EXPECTED_MONSTER_FIELDS`).\"\"\"\n",
    "        response = requests.get(monster_url, headers=self.headers)\n",
    "        monster_soup = BeautifulSoup(response.content)\n",
    "        tbody = monster_soup.select(\"tbody\")\n",
    "        soup = BeautifulSoup(str(tbody), \"html.parser\")\n",
    "        rows = soup.find_all(\"tr\")\n",
    "\n",
    "        monster_data = {}\n",
    "        facts = []\n",
    "\n",
    "        for row in rows:\n",
    "            th = row.find(\"th\")\n",
    "            tds = row.find_all(\"td\")\n",
    "\n",
    "            # symbol\n",
    "            if th and \"colspan\" in th.attrs:\n",
    "                name_tag = th.find(\"span\", class_=\"nhsym\")\n",
    "                if name_tag:\n",
    "                    monster_data[\"symbol\"] = name_tag.text.strip()\n",
    "\n",
    "            # Regular stat fields\n",
    "            elif th and len(tds) == 1:\n",
    "                key = th.get_text(strip=True).replace(\" (by normal means)\", \"\")\n",
    "                value = tds[0].get_text(strip=True)\n",
    "                monster_data[key] = value\n",
    "\n",
    "            # Bullet-point facts\n",
    "            elif tds and tds[0].find(\"ul\"):\n",
    "                for li in tds[0].find_all(\"li\"):\n",
    "                    fact = li.get_text(strip=True)\n",
    "                    if fact:\n",
    "                        facts.append(fact)\n",
    "\n",
    "            # External reference\n",
    "            elif len(tds) == 2 and \"Reference\" in tds[0].text:\n",
    "                monster_data[\"Reference\"] = tds[1].find(\"a\")[\"href\"]\n",
    "\n",
    "        if facts:\n",
    "            monster_data[\"facts\"] = facts\n",
    "\n",
    "        return {k: v for k, v in monster_data.items() if k in EXPECTED_MONSTER_FIELDS}\n",
    "    \n",
    "    # --- Items --- \n",
    "    def _get_item_classes_url_dict(self) -> Dict[str, str]:\n",
    "        items_urls = {}\n",
    "        for sub_li in self.items_url_soup.select(\"li\"):\n",
    "            try:\n",
    "                link = sub_li.find(\"a\")\n",
    "                text = sub_li.get_text(strip=True)\n",
    "                symbol = text.split(\"–\")[0]\n",
    "                name = text.split(\"–\")[1]\n",
    "                url = link.get(\"href\")\n",
    "                items_urls[name] = {\"symbol\": symbol, \"url\": url}\n",
    "            except IndexError:\n",
    "                continue\n",
    "        return items_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93817cff",
   "metadata": {},
   "source": [
    "# ***Monsters*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35667f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 90990.63it/s]\n"
     ]
    }
   ],
   "source": [
    "scraper = NetHackScraper(is_local=True)\n",
    "monster_url_dict = scraper._get_monsters_url_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de24460",
   "metadata": {},
   "outputs": [],
   "source": [
    "monsters = {}\n",
    "for k in tqdm(monster_url_dict.keys()):\n",
    "    monsters[k] = scraper.parse_monster_info(monster_url_dict[k])\n",
    "\n",
    "if monsters != {}:\n",
    "    pd.DataFrame(monsters).to_json(\"dataset/nethack_monsters\")\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT} Collectedz {len(monsters.keys())} Monster stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7d8e6",
   "metadata": {},
   "source": [
    "# ***Items*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "840e1d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Coins': {'symbol': '$', 'url': '/index.php/Zorkmid'},\n",
       " 'Amulets': {'symbol': '\"', 'url': '/index.php/Amulet'},\n",
       " 'Weapons': {'symbol': ')', 'url': '/index.php/Weapon'},\n",
       " 'Armor': {'symbol': '[', 'url': '/index.php/Armor'},\n",
       " 'Comestibles': {'symbol': '%', 'url': '/index.php/Comestible'},\n",
       " 'Scrolls': {'symbol': '?', 'url': '/index.php/Scroll'},\n",
       " 'Spellbooks': {'symbol': '+', 'url': '/index.php/Spellbook'},\n",
       " 'Potions': {'symbol': '!', 'url': '/index.php/Potion'},\n",
       " 'Rings': {'symbol': '=', 'url': '/index.php/Ring'},\n",
       " 'Wands': {'symbol': '/', 'url': '/index.php/Wand'},\n",
       " 'Tools': {'symbol': '(', 'url': '/index.php/Tool'},\n",
       " 'Gems/Stones': {'symbol': '*', 'url': '/index.php/Gem'},\n",
       " 'Boulders/Statues': {'symbol': '`', 'url': '/index.php/Boulder'},\n",
       " 'Iron balls': {'symbol': '0', 'url': '/index.php/Heavy_iron_ball'},\n",
       " 'Chains': {'symbol': '_', 'url': '/index.php/Iron_chain'},\n",
       " 'Venoms': {'symbol': '.', 'url': '/index.php/Venom'}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_classes_urls = scraper._get_item_classes_url_dict()\n",
    "item_classes_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item_info(self, item_url:str) -> Dict[str, Union[str | List[str]]]:\n",
    "        \"\"\"Collects a dictionary of monster properties (see `EXPECTED_MONSTER_FIELDS`).\"\"\"\n",
    "        response = requests.get(item_url, headers=self.headers)\n",
    "        monster_soup = BeautifulSoup(response.content)\n",
    "        tbody = monster_soup.select(\"tbody\")\n",
    "        soup = BeautifulSoup(str(tbody), \"html.parser\")\n",
    "        rows = soup.find_all(\"tr\")\n",
    "\n",
    "        monster_data = {}\n",
    "        facts = []\n",
    "\n",
    "        for row in rows:\n",
    "            th = row.find(\"th\")\n",
    "            tds = row.find_all(\"td\")\n",
    "\n",
    "            # symbol\n",
    "            if th and \"colspan\" in th.attrs:\n",
    "                name_tag = th.find(\"span\", class_=\"nhsym\")\n",
    "                if name_tag:\n",
    "                    monster_data[\"symbol\"] = name_tag.text.strip()\n",
    "\n",
    "            # Regular stat fields\n",
    "            elif th and len(tds) == 1:\n",
    "                key = th.get_text(strip=True).replace(\" (by normal means)\", \"\")\n",
    "                value = tds[0].get_text(strip=True)\n",
    "                monster_data[key] = value\n",
    "\n",
    "            # Bullet-point facts\n",
    "            elif tds and tds[0].find(\"ul\"):\n",
    "                for li in tds[0].find_all(\"li\"):\n",
    "                    fact = li.get_text(strip=True)\n",
    "                    if fact:\n",
    "                        facts.append(fact)\n",
    "\n",
    "            # External reference\n",
    "            elif len(tds) == 2 and \"Reference\" in tds[0].text:\n",
    "                monster_data[\"Reference\"] = tds[1].find(\"a\")[\"href\"]\n",
    "\n",
    "        if facts:\n",
    "            monster_data[\"facts\"] = facts\n",
    "\n",
    "        return {k: v for k, v in monster_data.items() if k in EXPECTED_MONSTER_FIELDS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b18e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd857cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58122965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# It's better to use the official API endpoint.\n",
    "API_URL = \"http://localhost:8080/api.php\"  # For your local container\n",
    "\n",
    "\n",
    "class NetHackScraper:\n",
    "    \"\"\"\n",
    "    A scraper for the NetHack Wiki that uses the official MediaWiki API\n",
    "    for robust and efficient data extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_local: bool):\n",
    "        self.api_url = API_URL if is_local else \"https://nethackwiki.com/api.php\"\n",
    "\n",
    "        # A dispatcher mapping item classes to their specific parsing functions.\n",
    "        # This is a clean and extensible way to handle different page structures.\n",
    "        self.category_parsers: Dict[str, Callable] = {\n",
    "            \"Monsters\": self._parse_monster_info,\n",
    "            \"Weapons\": self._parse_weapon_info,\n",
    "            \"Armor\": self._parse_armor_info,\n",
    "            # Add other parsers here, e.g., \"Potions\": self._parse_consumable_info\n",
    "        }\n",
    "\n",
    "    def _make_api_request(self, params: Dict) -> Dict:\n",
    "        \"\"\"Helper function to make a request to the MediaWiki API.\"\"\"\n",
    "        params[\"format\"] = \"json\"\n",
    "        try:\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def get_pages_in_category(self, category_title: str) -> List[str]:\n",
    "        \"\"\"Gets a list of all page titles in a given category.\"\"\"\n",
    "        print(f\"Fetching pages from 'Category:{category_title}'...\")\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": f\"Category:{category_title}\",\n",
    "            \"cmlimit\": \"500\",  # Request max limit\n",
    "            \"cmtype\": \"page\",\n",
    "        }\n",
    "        data = self._make_api_request(params)\n",
    "        pages = [\n",
    "            page[\"title\"]\n",
    "            for page in data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        ]\n",
    "        print(f\"Found {len(pages)} pages.\")\n",
    "        return pages\n",
    "\n",
    "    def get_pages_content(self, page_titles: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Fetches the raw wikitext content for a list of pages in a single batch.\n",
    "        This is much more efficient than one request per page.\n",
    "        \"\"\"\n",
    "        content_dict = {}\n",
    "        # Batch requests to avoid URLs that are too long\n",
    "        for i in tqdm(range(0, len(page_titles), 50), desc=\"Batch fetching content\"):\n",
    "            batch_titles = page_titles[i : i + 50]\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"rvprop\": \"content\",\n",
    "                \"titles\": \"|\".join(batch_titles),\n",
    "                \"redirects\": 1,  # Follow redirects\n",
    "            }\n",
    "            data = self._make_api_request(params)\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            for page_id, page_data in pages.items():\n",
    "                title = page_data.get(\"title\")\n",
    "                content = page_data.get(\"revisions\", [{}])[0].get(\"*\")\n",
    "                if title and content:\n",
    "                    content_dict[title] = content\n",
    "        return content_dict\n",
    "\n",
    "    # --- Custom Parsers for Different Entity Types ---\n",
    "\n",
    "    def _parse_monster_info(self, title: str, wikitext: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parses the wikitext of a monster page to extract structured data\n",
    "        from the 'Monster' infobox template.\n",
    "        \"\"\"\n",
    "        # Regex to find key-value pairs inside the {{Monster ... }} template\n",
    "        # This is more robust than parsing HTML.\n",
    "        infobox_match = re.search(r\"\\{\\{monster\\s*\\|([\\s\\S]*?)\\}\\}\", wikitext)\n",
    "        print(infobox_match)\n",
    "        if not infobox_match:\n",
    "            return {}\n",
    "\n",
    "        infobox_content = infobox_match.group(1)\n",
    "        monster_data = {\"name\": title, \"type\": \"monster\"}\n",
    "        \n",
    "        # Regex to find '| key = value' lines\n",
    "        pattern = re.compile(r\"\\|\\s*([^=]+?)\\s*=\\s*(.*)\")\n",
    "        matches = pattern.findall(infobox_content)\n",
    "\n",
    "        for key, value in matches:\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            # Clean up wikitext links like [[stone]] -> stone\n",
    "            value = re.sub(r\"\\[\\[([^|\\]]+?)(?:\\|[^\\]]+)?\\]\\]\", r\"\\1\", value)\n",
    "            monster_data[key] = value\n",
    "\n",
    "        return monster_data\n",
    "\n",
    "    def _parse_weapon_info(self, title: str, wikitext: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parses the wikitext of a weapon page to extract structured data.\n",
    "        (This is a placeholder - you would implement regex or string parsing here).\n",
    "        \"\"\"\n",
    "        return {\"name\": title, \"type\": \"weapon\", \"damage\": \"1d6\", \"cost\": 10}\n",
    "\n",
    "    def _parse_armor_info(self, title: str, wikitext: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parses the wikitext of an armor page.\n",
    "        \"\"\"\n",
    "        return {\"name\": title, \"type\": \"armor\", \"ac\": 3, \"material\": \"iron\"}\n",
    "\n",
    "    # --- Main Execution ---\n",
    "\n",
    "    def generate_dataset(self, save_path: str):\n",
    "        \"\"\"\n",
    "        Generates a dataset for all supported entity classes and saves it.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        for category, parser_func in self.category_parsers.items():\n",
    "            all_category_data = {}\n",
    "            page_titles = self.get_pages_in_category(category)\n",
    "            if not page_titles:\n",
    "                continue\n",
    "\n",
    "            pages_content = self.get_pages_content(page_titles)\n",
    "\n",
    "            for title, content in tqdm(\n",
    "                pages_content.items(), desc=f\"Parsing {category}\"\n",
    "            ):\n",
    "                item_data = parser_func(title, content)\n",
    "                if item_data:\n",
    "                    all_category_data[title] = item_data\n",
    "\n",
    "            print(len(all_category_data.keys()))\n",
    "            if all_category_data:\n",
    "                output_file = os.path.join(save_path, f\"nethack_{category.lower()}.jsonl\")\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    for item_name, data in all_category_data.items():\n",
    "                        f.write(json.dumps({item_name: data}) + \"\\n\")\n",
    "                print(\n",
    "                    f\"\\nSuccessfully saved {len(all_category_data)} entities to {output_file}\"\n",
    "                )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper = NetHackScraper(is_local=True)\n",
    "#     scraper.generate_dataset(save_path=\"datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35acf600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pages from 'Category:Monsters'...\n",
      "Found 332 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch fetching content: 100%|██████████| 7/7 [00:00<00:00, 21.06it/s]\n"
     ]
    }
   ],
   "source": [
    "titles = scraper.get_pages_in_category(\"Monsters\")\n",
    "wikitext = scraper.get_pages_content(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ea9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = wikitext[\"Giant ant\"]\n",
    "infobox_match = re.search(r\"\\{\\{monster\\s*\\|([\\s\\S]*?)\\}\\}\", wk)\n",
    "infobox_content = infobox_match.group(1)\n",
    "\n",
    "pattern = re.compile(r\"\\|\\s*([^=]+?)\\s*=\\s*(.*)\")\n",
    "matches = pattern.findall(infobox_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cefc011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{monster\\n |name=giant ant\\n |difficulty=4\\n |level=2\\n |experience=20\\n |speed=18\\n |AC=3\\n |MR=0\\n |align=0\\n |frequency=3\\n |genocidable=yes\\n |attacks=[[Bite]] 1d4 [[Physical damage|physical]]\\n |weight=10\\n |nutr=10\\n |size=tiny\\n |resistances=none\\n |resistances conveyed=none\\n |attributes={{attributes|A giant ant|sgroup=1|animal=1|nohands=1|oviparous=1|carnivore=1|hostile=1}}\\n |reference=[https://github.com/NetHack/NetHack/blob/NetHack-3.6.7_Released/src/monst.c#L108 NetHack 3.6.7 - src/monst.c, line 108]\\n}}\\n{{alternate tilesets|giant ant}}\\nA \\'\\'\\'giant ant\\'\\'\\', {{monsym|giant ant}}, is a type of [[monster]] that appears in \\'\\'[[NetHack]]\\'\\'. It is a [[carnivorous]] and [[oviparous]] [[animal]] that is the most basic monster of the [[ant or other insect]] [[monster class]]. Despite being the weakest among its group, the giant ant is still a frequent cause of early deaths due to its [[speed]] and tendency to appear in groups.\\n\\nA giant ant has a single [[bite]] attack.\\n\\n==Generation==\\nRandomly generated giant ants are always created hostile, and may appear in small [[group]]s of 2-4.\\n\\nAn [[anthole]] may be populated entirely by giant ants.{{refsrc|src/mkroom.c|500|version=NetHack 3.6.7}}\\n\\nHostile giant ants can be generated by the [[summon insects]] [[monster spell]].{{refsrc|src/mcastu.c|589|version=NetHack 3.6.7}}\\n\\nGiant ants can appear among the {{white|a}} that are part of the first [[quest]] monster class for [[Valkyrie]]s and make up {{frac|24|175}} of the monsters that are randomly generated on the [[Valkyrie quest]].\\n\\n==Strategy==\\nGiant ants are very fast monsters that are one of the major obstacles to early characters, and are adept at running down the under-prepared, especially with their 18 speed: a giant ant\\'s [[AC]] of 3 can also make them tricky to hit for characters that do not have trained-up or enchanted weapons. If you spot an oncoming ant swarm, you can avoid being surrounded by getting into a [[corridor]] as soon as possible, or else positioning yourself to block a corridor that ants are traveling through&mdash;you can also use [[door]]s to block off pursuing ants if you can close them quickly. Take note of the location of up and/or down [[stairs]], and be prepared to retreat to said stairs if things get out of hand; avoid being [[burdened]] if at all reasonable to maximize your own speed. \\n\\n[[Ranged weapon]]s, along with offensive spells and [[wand]]s, are your best bet for handling a giant ant or a swarm of them before they can hem you in, and magic in general also works against giant ants, since they have no [[Magic resistance (monster)|MR score]]. All of these attack methods are especially effective if you can contain the giant ants to a corridor: though their speed is still enough to make them a nuisance, they are not as threatening in one-on-one combat. A means of putting the ants to [[sleep]] or slowing them down, such as wands of {{wand of|sleep|and=1|slow monster}}, can turn the tide against a lined-up swarm, and may even give you the room to escape a group of ants if you cannot reliably fight them off. Wands of {{wand of|magic missile}} can thin out a group of lined-up ants. [[Elbereth]] can reliably drive off attacking ants as well and may leave them open to your ranged attack of choice.\\n\\nKeep any [[escape item]]s handy if all else fails, particularly ones that can get you to the stairs or off the level faster. In addition to wands of sleep and slow monster, [[scroll]]s of {{? of|teleportation}} and wands of {{wand of|teleportation}} can also put distance between you and the ants; cursed scrolls of teleportation can warp you to another level, but are not reliable. Cursed [[spellbooks]] can sometimes cause teleportation, but should only be used as a last resort. Upon escaping a group of ants, it may be wise to steer clear until you are better prepared to handle them.\\n\\nA hero that reaches the mid-game in particular is usually equipped enough to handle giant ants from that point forward: ants created by the summon insects spell in particular are usually encountered at a point where they are of no real trouble to most heroes, and can even serve as a useful physical buffer, e.g. against [[aligned priest]]s.\\n\\n==History==\\nThe giant ant first appears in Hack 1.21 and Hack for PDP-11, which are based on [[Jay Fenlason\\'s Hack]], and is included in the initial bestiary for [[Hack 1.0]]. From these versions to [[NetHack 2.3e]], the giant ant uses the {{white|A}} glyph and is more similar in behavior to the modern [[soldier ant]]. [[NetHack 3.0.0]] introduces the ant or other insect monster class, moving the giant ant to {{white|a}} and introducing the soldier ant alongside it.\\n\\n==Origin==\\n{{wikipedia|Ant}}\\nGiant ants&ndash;specifically, ants or ant-like creatures that are human-sized or larger&ndash;are a popular form of insectoid in science-fiction media, codified by works such as the 1905 H. G. Wells short story \\'\\'Empire of the Ants\\'\\' and the 1954 film \\'\\'Them!\\'\\', and other notable examples include the Bugs of \\'\\'Starship Troopers\\'\\', the Formics of \\'\\'Ender\\'s Game\\'\\', and the ants that accompany Marvel Comics super hero Ant-Man. Like other ants in various forms of fiction, they are commonly portrayed as a hive species, which is a common misconception regarding ant colonies.\\n\\nThe giant ant of \\'\\'NetHack\\'\\' is also derived from \\'\\'[[Dungeons & Dragons]]\\'\\', where {{frac|9|10}} of ants encountered will be workers. They can appear in groups of up to 100: twice as many can appear in giant ant nests, including 1 \"warrior\" per 5 workers; these nests also contain a single queen whose chamber holds the nest\\'s treasures, and the egg chamber is guarded by 5 warriors and 5-50 workers. If the queen is killed, the other ants will become confused for six melee rounds before fleeing the nest.\\n\\nWhile the giant ant does not seem to be based on any particular species, one of the largest known ant species is \\'\\'[[Wikipedia:Dinoponera|Dinoponera]]\\'\\', a South American genus of ant in the subfamily Ponerinae that are commonly called \"tocandiras\" or \"giant Amazonian ants\": While less known overall than the [[bullet ant]], females of the genus can reach a total body length of 3–4 cm (1.2–1.6 in) or more.\\n\\n==Variants==\\n===dNetHack===\\nIn [[dNetHack]], [[notdNetHack]] and [[notnotdNetHack]], giant ants are a lawful species.\\n\\n==Encyclopedia entry==\\n{{encyclopedia|<poem>\\nThis giant variety of the ordinary ant will fight just as\\nfiercely as its small, distant cousin.  Various varieties\\nexist, and they are known and feared for their relentless\\npersecution of their victims.\\n</poem>}}\\n\\n==References==\\n<references/>\\n{{nethack-367}}\\n[[Category:Monsters]]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf64b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('difficulty', '4'),\n",
       " ('level', '2'),\n",
       " ('experience', '20'),\n",
       " ('speed', '18'),\n",
       " ('AC', '3'),\n",
       " ('MR', '0'),\n",
       " ('align', '0'),\n",
       " ('frequency', '3'),\n",
       " ('genocidable', 'yes'),\n",
       " ('attacks', '[[Bite]] 1d4 [[Physical damage|physical]]'),\n",
       " ('weight', '10'),\n",
       " ('nutr', '10'),\n",
       " ('size', 'tiny'),\n",
       " ('resistances', 'none'),\n",
       " ('resistances conveyed', 'none'),\n",
       " ('attributes',\n",
       "  '{{attributes|A giant ant|sgroup=1|animal=1|nohands=1|oviparous=1|carnivore=1|hostile=1')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c85cb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(titles, wikitext)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(scraper\u001b[38;5;241m.\u001b[39m_parse_monster_info(title, text))\n\u001b[0;32m----> 3\u001b[0m     infobox_match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43mmonster\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m|([\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mS]*?)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwikitext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/re.py:200\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "for title, text in list(zip(titles, wikitext)):\n",
    "    print(scraper._parse_monster_info(title, text))\n",
    "    infobox_match = re.search(r\"\\{\\{monster\\s*\\|([\\s\\S]*?)\\}\\}\", wikitext)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hercules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
